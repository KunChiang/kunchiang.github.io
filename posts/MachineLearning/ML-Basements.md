# 激活函数

## 什么是激活函数？&& 为什么要用激活函数？

激活函数就是一个非线性函数，作用于网络层的输出，增加神经网络的非线性

如果不用激活函数，神经网络中的每一层的输出都是输入的线性函数，激活函数就给神经网络引入了非线性因素，使得网络可以逼近任意非线性函数

## 激活函数有那些？

饱和激活函数：
- Sigmoid: (0,1)
    - 容易导致梯度消失
    - 非原点中心
    - 计算费时
- tanh: (-1, 1)
    - 双曲正切
    - 容易导致梯度消失

非饱和激活函数
- ReLu: max(0, x)
    - 基本解决梯度消失的问题
    - 收敛更快
    - 负区间输入的输出始终为0，不能学习
- leaky ReLu: (ax, x)
    - 在负区间引入一个leak值，使得负区间的输入也能够学习

## 如何选择合适的激活函数？

二分类可以选择sigmoid，默认选择ReLu，或者Leak ReLu

## 激活函数以0为中心的问题？

激活函数不以0为中心，导致目标函数关于参数的梯度总是为正或者负，导致参数总是在同一方向上变化，导致收敛速度慢

https://blog.csdn.net/weixin_43835911/article/details/89294613

# 梯度消失和梯度爆炸

## 原因

因为网络参数的是更新是依靠链式法则进行梯度传导，链式法则的连乘结构就有可能导致梯度消失或者梯度爆炸的出现

## 出现情况
- 深层网络
- 不合适的损失函数
- 权值初始化太大

## 表现
梯度消失的表现：

- 模型无法从训练数据中获得更新，损失几乎保持不变。

梯度爆炸的表现：

- 模型型不稳定，更新过程中的损失出现显著变化。
- 训练过程中，模型损失变成 NaN。

## 解决方法：

- 降低学习率
- 使用relu激活函数
- 使用BatchNorm
- 残差网络
- LSTM

## 参考
- [参考博客1](https://blog.csdn.net/qq_25737169/article/details/78847691)
- [参考博客2](https://blog.csdn.net/vivian_ll/article/details/100919715)

## loss突然变nan的原因？

- 脏数据
- 学习速率过大
- 可能是损失函数的问题

# 损失函数

## 什么是损失函数

- 损失函数（Loss Function）：是定义在单个样本上的，是指一个样本的误差。
- 代价函数（Cost Function）：是定义在整个训练集上的，是所有样本误差的平均，也就是所有损失函数值的平均。
- 目标函数（Object Function）：是指最终需要优化的函数，一般来说是经验风险+结构风险，也就是（代价函数+正则化项）。


## 损失函数有哪些

回归损失函数：

- 均方误差（MSE）：差的平方再求平均
    - 平方之后放大分类错误的误差
    - 梯度更好计算
- 平均绝对误差（MAE）：差的绝对值再求平均
    - 无平方，更稳健
    - 梯度计算复杂些

分类损失函数：
- Hinge Loss/多分类 SVM 损失：
$$
max(0, 1-y_if(x_i)
$$
- 交叉熵损失函数/LR损失/对数损失：
$$
- \frac{1}{N} \sum_i^n y_i log f(x_i) + (1-y_i) log (1-f(x_i))
$$


其他：
- 指数损失函数（AdaBoost用）：
$$
\frac{1}{n}\sum exp(-y_if(x_i))
$$

## Softmax

将输入值映射到（0,1）区间，而这些值的和为1

$$
\frac{exp(x_i)}{\sum_j^n exp(x_j)}
$$

# 优化方法

## 优化方法“归一化”

1. 计算梯度
$$
g_t = \nabla f(w_t)
$$
2. 计算动量
$$
m_t = \phi(g_1, g_2, ..., g_t)

v_t = \psi(g_1, g_2, ..., g_t)
$$
3. 梯度下降
$$
\eta_t = \alpha \cdot m_t/\sqrt v_t
$$
4. 参数更新
$$
w_t = w_{t-1} - \eta_t
$$

## 梯度下降优化方法
- 标准梯度下降：沿着梯度下降方向不断更新模型参数，最小化代价函数
    - 下降速度慢
- 批量梯度下降：计算整个数据集的梯度，沿平均梯度最小的方向更新参数
- 随机梯度下降：随机选择一个样本的梯度来进行参数更新
- 小批量梯度下降：随机选择一个小batch的样本的平均梯度来更新参数

## 动量优化方法

加速梯度下降
- Momentum：引入一个积攒历史梯度信息的动量
- NAG：

## 自适应学习率优化方法
- AdaGard：自适应地为各个参数分配不同的学习率
- RMSProp
- Adam
- AdaDelta

### Adam的问题

- 可能不收敛：
    - SGD学习率恒定，但实际使用时会使用衰减策略，AdaGard使用累计的动量，学习率单调递减，都能最终收敛；
    - Adam和AdaDelta的二阶动量是固定时间窗口内的累计，可能导致学习率震荡，最后不一定收敛

## 如何选择优化方法
如果要快速验证模型效果，那就选择自适应地优化算法，如Adam；如果要上线，再用SGD加动量精细调优

## Tricks

-  制定一个合适的学习率衰减策略。可以使用定期衰减策略，比如每过多少个epoch就衰减一次；或者利用精度或者AUC等性能指标来监控，当测试集上的指标不变或者下跌时，就降低学习率。
-  数据集一定要充分的打散（shuffle）。这样在使用自适应学习率算法的时候，可以避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题。
-  考虑不同算法的组合。先用Adam进行快速下降，而后再换到SGD进行充分的调优。切换策略可以参考本文介绍的方法。

## Refer

- [Adam那么棒，为什么还对SGD念念不忘](https://cloud.tencent.com/developer/article/1143084)

# 参数初始化方法

- 影响梯度传播（可能会导致梯度消失）、训练速度

## 初始化方法

- 随机初始化
    - 随机抽取服从高斯分布或均匀分布
    - 可能会导致梯度消失
- Xavier初始化
    - 保持输入和输出的方差一致（服从相同的分布）
    - 只适用于线性激活函数
    - 适用于tanh，不适合ReLu
- He/Kaiming 初始化
    - 在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0（x负半轴中是不激活的），所以要保持variance不变，只需要在Xavier的基础上再除以2
- Kaiming
- Pre_train 初始化
- BatchNormalizaltion Layer